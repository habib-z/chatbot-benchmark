# -*- coding: utf-8 -*-
"""dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EjS9GZDOMJqRn0zRJUlEG4LGRiOagSos
"""

# !pip install -U ir_datasets sentence-transformers qdrant-client tqdm pytrec_eval

import ir_datasets, collections, tqdm, random, textwrap, statistics
wrap = lambda s: textwrap.fill(s, 88)

ds = ir_datasets.load("miracl/fa/dev")              # 632 queries / 210 k docs
docs = {d.doc_id: d.text for d in ds.docs_iter()}

queries = {q.query_id: q.text for q in ds.queries_iter()}
qrels   = collections.defaultdict(dict)             # qid → {doc: rel}
for qr in ds.qrels_iter():
    if qr.relevance > 0:                            # keep only positives
        qrels[qr.query_id][qr.doc_id] = 1           # binary

print(f"docs:{len(docs):,}  queries:{len(queries):,}  positives avg:"
      f"{statistics.mean(len(r) for r in qrels.values()):.2f}")

# eyeball helper

def show_random_query(pos_k=-1, neg_k=2):
    qid = random.choice(list(qrels.keys()))
    print("\n" + "="*100)
    print("Query:", wrap(queries[qid]), "\n")
    if pos_k < 0:
        pos_k = len(qrels[qid])
        print(f"pos num = {pos_k}")
    for did in random.sample(list(qrels[qid].keys()), k=min(pos_k, len(qrels[qid]))):
        print("[POS]", wrap(docs[did][:700]), "\n")
    neg_pool = [d for d in docs if d not in qrels[qid]]
    for did in random.sample(neg_pool, k=neg_k):
        print("[NEG]", wrap(docs[did][:700]), "\n")

show_random_query()

qid = random.choice(list(qrels.keys()))

list(qrels[qid].keys())

# embed and store Qdrant
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient, models

model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'
embedder = SentenceTransformer(model_name)           # ≈768-d





# !curl http://185.255.91.144:31633/health

client = QdrantClient(host="185.255.91.144", port=30081,timeout=60)
COLL = "miracl-fa"
dim  = embedder.get_sentence_embedding_dimension()
client.recreate_collection(
    collection_name=COLL,
    vectors_config=models.VectorParams(size=dim, distance=models.Distance.COSINE)
)

BATCH = 256
ids, vecs, payloads = [], [], []
for i, (doc_id, text) in enumerate(tqdm.tqdm(docs.items(), desc="Embedding")):
    vec = embedder.encode(text, normalize_embeddings=True)

    # Create payload with original IDs
    payload = {
        "chunk_id": doc_id,  # e.g., "594#0"
        "base_doc_id": doc_id.split('#')[0]  # e.g., "594"
    }

    ids.append(i)  # integer ID for Qdrant
    vecs.append(vec)
    payloads.append(payload)

    if len(ids) == BATCH:
        client.upsert(
            collection_name=COLL,
            points=models.Batch(ids=ids, vectors=vecs, payloads=payloads)
        )
        ids, vecs, payloads = [], [], []
# final flush
if ids:
    client.upsert(collection_name=COLL, points=models.Batch(ids=ids, vectors=vecs))

import numpy as np, pytrec_eval, tqdm

TOP_K = 10                    # change k here—should match your prompt budget

run = {}                      # {qid: {doc: score}}
for qid, qtext in tqdm.tqdm(queries.items(), desc="Retrieving"):
    qvec = embedder.encode(qtext, normalize_embeddings=True)
    res  = client.search(collection_name=COLL, query_vector=qvec,
                         limit=TOP_K, with_vectors=False)
    run[qid] = {point.id: 1 - point.score for point in res}     # higher better

# ground-truth dict in pytrec_eval format
gt = {qid: rels for qid, rels in qrels.items()}

metrics = {'recall_%d' % TOP_K,
           'P_%d'      % TOP_K,
           'ndcg_cut_%d' % TOP_K}

evaluator = pytrec_eval.RelevanceEvaluator(gt, metrics)
scores = evaluator.evaluate(run)

R   = np.mean([s[f"recall_{TOP_K}"]     for s in scores.values()])
P   = np.mean([s[f"P_{TOP_K}"]          for s in scores.values()])
nDCG= np.mean([s[f"ndcg_cut_{TOP_K}"]   for s in scores.values()])
print(f"Recall@{TOP_K}:{R:.3f}  Precision@{TOP_K}:{P:.3f}  nDCG@{TOP_K}:{nDCG:.3f}")